from pathlib import Path
from sentence_transformers import SentenceTransformer, models

MODEL_PATH = Path("local_models/intfloat/multilingual-e5-small")

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
REQUIRED = ["config.json", "tokenizer_config.json", "pytorch_model.bin", "sentencepiece.bpe.model", "modules.json"]
missing = [f for f in REQUIRED if not (MODEL_PATH / f).exists()]
if missing:
    raise RuntimeError(f"âŒ ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ñ„Ğ°Ğ¹Ğ»Ñ‹: {missing}")

# Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ
print("ğŸ“¦ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ SentenceTransformer Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹:", MODEL_PATH)
word_embedding_model = models.Transformer(str(MODEL_PATH), do_lower_case=True)
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())

model = SentenceTransformer(modules=[word_embedding_model, pooling_model])

def generate_embedding(text: str) -> list:
    return model.encode(f"passage: {text}").tolist()
